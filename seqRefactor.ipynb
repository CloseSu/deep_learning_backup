{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'cmn.txt'\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>chinses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>嗨。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>你好。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run.</td>\n",
       "      <td>你用跑的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wait!</td>\n",
       "      <td>等等！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hello!</td>\n",
       "      <td>你好。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  english chinses\n",
       "0     Hi.      嗨。\n",
       "1     Hi.     你好。\n",
       "2    Run.   你用跑的。\n",
       "3   Wait!     等等！\n",
       "4  Hello!     你好。"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"cmn.txt\", sep='\\t', header=None, names=[\"english\",\"chinses\"])\n",
    "data = data[:10000]\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"english\"] = data[\"english\"].apply(lambda x: \"\\t \" + x + \" \\n\")\n",
    "data[\"chinses\"] = data[\"chinses\"].apply(lambda x: \"\\t \" + x + \" \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenizer(df):\n",
    "    tokenizer = Tokenizer(num_words=8000, char_level=True)\n",
    "    dcaptions = df.values\n",
    "    tokenizer.fit_on_texts(dcaptions)\n",
    "    vocab_size = len(tokenizer.word_index) + 1    \n",
    "    dtexts = tokenizer.texts_to_sequences(dcaptions)    \n",
    "    maxlen = np.max([len(text) for text in dtexts])\n",
    "    index_word = dict([(index,word) for word, index in tokenizer.word_index.items()])\n",
    "    word_index = dict([(word,index) for word, index in tokenizer.word_index.items()])  \n",
    "        \n",
    "    return tokenizer, dtexts, vocab_size, maxlen, index_word, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76, 34)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_eng, dtexts_eng, vocab_size_english, maxlen_english, index_word_english, word_index_english = word_tokenizer(data[\"english\"])\n",
    "data[\"english_token\"] = dtexts_eng\n",
    "vocab_size_english, maxlen_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2624, 24)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_chi, dtexts_chi, vocab_size_chinses, maxlen_chinses, index_word_chinses, word_index_chinses = word_tokenizer(data[\"chinses\"])\n",
    "data[\"chinese_token\"] = dtexts_chi\n",
    "vocab_size_chinses, maxlen_chinses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>chinses</th>\n",
       "      <th>english_token</th>\n",
       "      <th>chinese_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\t Hi. \\n</td>\n",
       "      <td>\\t 嗨。 \\n</td>\n",
       "      <td>[8, 1, 31, 7, 13, 1, 9]</td>\n",
       "      <td>[2, 1, 1265, 4, 1, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\t Hi. \\n</td>\n",
       "      <td>\\t 你好。 \\n</td>\n",
       "      <td>[8, 1, 31, 7, 13, 1, 9]</td>\n",
       "      <td>[2, 1, 7, 26, 4, 1, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\t Run. \\n</td>\n",
       "      <td>\\t 你用跑的。 \\n</td>\n",
       "      <td>[8, 1, 57, 16, 10, 13, 1, 9]</td>\n",
       "      <td>[2, 1, 7, 117, 261, 6, 4, 1, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\t Wait! \\n</td>\n",
       "      <td>\\t 等等！ \\n</td>\n",
       "      <td>[8, 1, 32, 5, 7, 4, 44, 1, 9]</td>\n",
       "      <td>[2, 1, 187, 187, 90, 1, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\t Hello! \\n</td>\n",
       "      <td>\\t 你好。 \\n</td>\n",
       "      <td>[8, 1, 31, 2, 14, 14, 3, 44, 1, 9]</td>\n",
       "      <td>[2, 1, 7, 26, 4, 1, 3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        english      chinses                       english_token  \\\n",
       "0     \\t Hi. \\n     \\t 嗨。 \\n             [8, 1, 31, 7, 13, 1, 9]   \n",
       "1     \\t Hi. \\n    \\t 你好。 \\n             [8, 1, 31, 7, 13, 1, 9]   \n",
       "2    \\t Run. \\n  \\t 你用跑的。 \\n        [8, 1, 57, 16, 10, 13, 1, 9]   \n",
       "3   \\t Wait! \\n    \\t 等等！ \\n       [8, 1, 32, 5, 7, 4, 44, 1, 9]   \n",
       "4  \\t Hello! \\n    \\t 你好。 \\n  [8, 1, 31, 2, 14, 14, 3, 44, 1, 9]   \n",
       "\n",
       "                     chinese_token  \n",
       "0            [2, 1, 1265, 4, 1, 3]  \n",
       "1           [2, 1, 7, 26, 4, 1, 3]  \n",
       "2  [2, 1, 7, 117, 261, 6, 4, 1, 3]  \n",
       "3       [2, 1, 187, 187, 90, 1, 3]  \n",
       "4           [2, 1, 7, 26, 4, 1, 3]  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seq_data(row):\n",
    "    eng_x, chi_x, chi_y = [], [], []\n",
    "    english_token = row[\"english_token\"]\n",
    "    chinese_token = row[\"chinese_token\"]\n",
    "    for i in range(1,maxlen_english):\n",
    "        if i < len(english_token):\n",
    "            eng_text = english_token[:i]\n",
    "            eng_text = pad_sequences([eng_text],maxlen=maxlen_english).flatten()\n",
    "            eng_x.append(eng_text)\n",
    "        else:\n",
    "            eng_x.append(np.zeros(maxlen_english))\n",
    "    for i in range(1,maxlen_chinses):\n",
    "        if i < len(chinese_token):\n",
    "            chi_text, chi_target = chinese_token[:i], chinese_token[i]\n",
    "            chi_text = pad_sequences([chi_text],maxlen=maxlen_chinses).flatten()\n",
    "            chi_target = to_categorical(chi_target,num_classes = vocab_size_chinses)       \n",
    "            chi_x.append(chi_text)\n",
    "            chi_y.append(chi_target)\n",
    "        else:\n",
    "            chi_x.append(np.zeros(maxlen_chinses))\n",
    "            chi_y.append(np.zeros(vocab_size_chinses))\n",
    "            \n",
    "   \n",
    "    return eng_x, chi_x, chi_y              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(df, batch_size):\n",
    "    batch_eng_x, batch_chi_x, batch_chi_y = [], [], []\n",
    "    count = 0\n",
    "    while True:\n",
    "        for i,row in df.iterrows():\n",
    "            count += 1\n",
    "            eng_x, chi_x, chi_y = get_seq_data(row)\n",
    "            batch_eng_x.append(eng_x)\n",
    "            batch_chi_x.append(chi_x)\n",
    "            batch_chi_y.append(chi_y)\n",
    "\n",
    "            if count == batch_size:\n",
    "                batch_eng_x = np.array(batch_eng_x)\n",
    "                batch_chi_x = np.array(batch_chi_x)\n",
    "                batch_chi_y = np.array(batch_chi_y)\n",
    "                yield [[batch_eng_x, batch_chi_x],batch_chi_y]\n",
    "                batch_eng_x, batch_chi_x, batch_chi_y = [], [], []\n",
    "                count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "gerato = data_generator(data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(gerato)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((64, 33, 34), (64, 23, 24), (64, 23, 2624))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][0].shape, a[0][1].shape, a[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256\n",
    "encoder_inputs = Input(shape=(None, maxlen_english))\n",
    "encoder_outputs, state_h, state_c = LSTM(latent_dim, return_state=True)(encoder_inputs)\n",
    "\n",
    "decoder_inputs = Input(shape=(None, maxlen_chinses))\n",
    "decoder_outputs, _, _ =  LSTM(latent_dim, return_sequences=True, return_state=True)(decoder_inputs, initial_state=[state_h, state_c])\n",
    "\n",
    "decoder_outputs = Dense(vocab_size_chinses, activation='softmax')(decoder_outputs)\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, None, 34)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, None, 24)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, 256), (None, 297984      input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, None, 256),  287744      input_8[0][0]                    \n",
      "                                                                 lstm_3[0][1]                     \n",
      "                                                                 lstm_3[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 2624)   674368      lstm_4[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,260,096\n",
      "Trainable params: 1,260,096\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " - 13s - loss: 1.8816\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizers.rmsprop(lr=1e-3), \n",
    "              loss='categorical_crossentropy')\n",
    "model_history = model.fit_generator(data_generator(data, batch_size),                                               \n",
    "                                    steps_per_epoch= data.shape[0]/batch_size,                                   \n",
    "                                    epochs=1, \n",
    "                                    verbose=2   \n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  model.save('s2sv5.h5')\n",
    "model = load_model('s2sv5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "eng_input (InputLayer)          (None, None, 34)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "chi_input (InputLayer)          (None, None, 24)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "endoder (LSTM)                  [(None, 256), (None, 297984      eng_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "decoder (LSTM)                  [(None, None, 256),  287744      chi_input[0][0]                  \n",
      "                                                                 endoder[0][1]                    \n",
      "                                                                 endoder[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 2624)   674368      decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,260,096\n",
      "Trainable params: 1,260,096\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model version\n",
    "latent_dim = 256\n",
    "encoder_inputs = model.input[0]\n",
    "_, encoder_statesh, encoder_statesc = model.layers[2].output\n",
    "encoder_model = Model(encoder_inputs, [encoder_statesh, encoder_statesc])\n",
    "\n",
    "decoder_inputs = model.input[1]\n",
    "decoder_states_inputs = [Input(shape=(latent_dim,)), decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = model.layers[3](decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_outputs = model.layers[4](decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs,[decoder_outputs] + [state_h, state_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_model = Model(encoder_inputs, [encoder_states])\n",
    "# decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "# decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "# decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "# decoder_states = [state_h, state_c]\n",
    "# decoder_outputs = decoder_dense(decoder_outputs)\n",
    "# decoder_model = Model([decoder_inputs] + decoder_states_inputs,[decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(np.array([input_seq]))\n",
    "    decoded_sentence = \"\\t\"\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        \n",
    "        sequence = tokenizer_chi.texts_to_sequences([decoded_sentence])[0]\n",
    "        sequence = np.array([pad_sequences([sequence],maxlen_chinses)])\n",
    "        \n",
    "        output_tokens, h, c = decoder_model.predict([sequence] + states_value)\n",
    "   \n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_word_chinses[sampled_token_index]       \n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > maxlen_chinses):\n",
    "            stop_condition = True\n",
    "\n",
    "        states_value = [h, c]    \n",
    "       \n",
    "              \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: Hi.\n",
      "Decoded sentence: 你好。\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: 你好。\n",
      "Input sentence: Run.\n",
      "Decoded sentence: 你用跑的。\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: 等等！\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: 你好。\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(5):   \n",
    "    eng_x, chi_x, chi_y = get_seq_data(data.iloc[seq_index])\n",
    "    decoded_sentence = decode_sequence(eng_x)\n",
    "    decoded_sentence = decoded_sentence.strip()\n",
    "    print('Input sentence:', data.iloc[seq_index][\"english\"].strip())\n",
    "    try:\n",
    "        print('Decoded sentence:', decoded_sentence)\n",
    "    except:       \n",
    "        print('Decoded sentence:', decoded_sentence.encode('ascii', 'replace'))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
